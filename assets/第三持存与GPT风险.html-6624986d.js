const t=JSON.parse('{"key":"v-5d346614","path":"/random/%E7%AC%AC%E4%B8%89%E6%8C%81%E5%AD%98%E4%B8%8EGPT%E9%A3%8E%E9%99%A9.html","title":"第三持存与GPT风险","lang":"zh-CN","frontmatter":{"lang":"zh-CN","title":"第三持存与GPT风险","description":"斯蒂格勒如何看待GPT?","head":[["meta",{"property":"og:url","content":"https://oshibuki.github.io/random/%E7%AC%AC%E4%B8%89%E6%8C%81%E5%AD%98%E4%B8%8EGPT%E9%A3%8E%E9%99%A9.html"}],["meta",{"property":"og:site_name","content":"Oshibuki的数字城堡"}],["meta",{"property":"og:title","content":"第三持存与GPT风险"}],["meta",{"property":"og:description","content":"斯蒂格勒如何看待GPT?"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-05-08T00:41:34.000Z"}],["meta",{"property":"article:author","content":"Oshibuki"}],["meta",{"property":"article:modified_time","content":"2023-05-08T00:41:34.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"第三持存与GPT风险\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-05-08T00:41:34.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Oshibuki\\",\\"url\\":\\"https://oshibuki.github.io\\"}]}"]]},"headers":[],"git":{"createdTime":1683506494000,"updatedTime":1683506494000,"contributors":[{"name":"Oshibuki","email":"qerlkkjdfalfjelj12.adf2@gmail.com","commits":1}]},"readingTime":{"minutes":10.67,"words":3201},"filePathRelative":"random/第三持存与GPT风险.md","localizedDate":"2023年5月8日","excerpt":"<h1> 第三持存与GPT风险</h1>\\n<p>“在这个世界上，最难的事情就是知道如何做一件事，然后看着别人做错了却不发表评论。”这句话出自西奥多·H·怀特，一位普利策奖获得者、美国记者和历史学家，他捕捉到了许多专家在目睹他们的知识领域被误用或误解时的挫败感。其中一个最近引起了很多关注和争议的领域就是大型语言模型（LLMs）的领域，例如GPT-3，它可以根据一个提示或一个查询，生成几乎任何主题的自然、类人的文本。这些模型源于自然语言处理的领域，它们已经在大量的文本数据上进行了训练，这些数据通常是从互联网上抓取的，并且在各种任务中表现出了令人印象深刻的能力，例如写作文、总结文章、撰写电子邮件，甚至创建聊天机器人。然而，随着这些非凡的成就，也带来了需要由研究人员、开发人员、用户和政策制定者共同仔细考虑和解决的重大风险和挑战。</p>"}');export{t as data};
