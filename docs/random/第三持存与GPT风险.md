---
lang: zh-CN
title: 第三持存与GPT风险
description: 斯蒂格勒如何看待GPT?
---

# 第三持存与GPT风险

“在这个世界上，最难的事情就是知道如何做一件事，然后看着别人做错了却不发表评论。”这句话出自西奥多·H·怀特，一位普利策奖获得者、美国记者和历史学家，他捕捉到了许多专家在目睹他们的知识领域被误用或误解时的挫败感。其中一个最近引起了很多关注和争议的领域就是大型语言模型（LLMs）的领域，例如GPT-3，它可以根据一个提示或一个查询，生成几乎任何主题的自然、类人的文本。这些模型源于自然语言处理的领域，它们已经在大量的文本数据上进行了训练，这些数据通常是从互联网上抓取的，并且在各种任务中表现出了令人印象深刻的能力，例如写作文、总结文章、撰写电子邮件，甚至创建聊天机器人。然而，随着这些非凡的成就，也带来了需要由研究人员、开发人员、用户和政策制定者共同仔细考虑和解决的重大风险和挑战。

在本文中，我们将探讨其中一些风险和挑战，首先简要介绍由乔治·斯蒂格勒提出的第三种持有物的概念，他是一位诺贝尔经济学奖得主。斯蒂格勒认为，社会中有三种类型的持有物：物理持有物（如土地、建筑物和机器）、人类持有物（如技能、知识和声誉）和信息持有物（如数据、秘密和专利）。他声称，信息持有物通常是最有价值和最有影响力的持有物，因为它们可以给拥有它们的人带来比没有它们的人更大的优势。他还建议，信息持有物通常是由行业通过监管获得的，而监管是主要为了它们的利益而设计和运作的。斯蒂格勒的经济监管理论已经广泛地影响并应用于各个公共政策领域。

这与LLMs有什么关系呢？嗯，LLMs可以被看作是强大的信息持有物的生成者和消费者。它们可以从现有的数据中创建新的信息，或者从可用的数据中推断出隐藏的信息。它们还可以访问和处理大量的信息，这些信息可能对大多数人来说是无法获取或难以理解的。这意味着LLMs可能会影响社会中信息持有物的分布和动态，为不同的行为者和利益相关者创造新的机会和挑战。例如，LLMs可以实现新的沟通、教育、娱乐和创新形式，但它们也可能对隐私、安全、信任、质量和道德构成威胁。

在接下来的几段中，我们将更详细地讨论与LLMs相关的一些具体风险和挑战。我们将借鉴来自计算机科学、语言学、社会科学和环境科学的多学科专业知识和文献。我们还将提出一些可能的方法来减轻这些风险和挑战，或者利用LLMs为公共利益服务。我们的目标不是提供确定的答案或解决方案，而是提高读者对LLMs感兴趣或受到影响的人们的意识，并激发他们的讨论。我们希望这篇文章能作为一个有用的介绍和进一步探索和辩论这一重要话题的起点。


LLMs的主要风险之一是**偏见和公平性**。LLMs从大量的文本数据中学习，这些数据可能包含有偏见的内容。例如，文本数据可能反映了基于性别、种族、民族、宗教、性取向或其他社会类别的刻板印象、偏见或歧视。LLMs可能会在其输出中重现或放大这些偏见，导致某些群体或个人受到不公平或有害的影响。例如，LLMs可能会生成性别歧视、种族歧视、恐同或其他冒犯或不恰当的文本。它们也可能对某些群体的表现比其他群体差，这可能是由于数据不平衡或代表性不足造成的。偏见和公平性不仅是技术问题，也是道德和社会问题，因为它们涉及到正义、平等和人类尊严等问题。

与LLMs相关的另一个重大风险是**错误信息和虚假信息**。LLMs可以生成可能具有误导性或虚假性的文本，导致错误信息和虚假信息的传播。错误信息是无意间创建或分享的错误或不准确的信息。虚假信息是故意为了恶意目的而创建或分享的错误或误导性的信息。这两种类型的信息都可能对个人和社会造成负面影响，例如侵蚀信任、影响信念和行为、破坏民主以及危及公共卫生和安全。例如，LLMs可以用来创建虚假新闻文章、虚假评论、虚假社交媒体帖子、虚假学术论文或虚假历史文件。它们也可以用来操纵或冒充人们的声音、身份或观点。

LLMs涉及的第三个风险领域是**恶意使用**。LLMs可以被试图对他人造成伤害或获得不公平优势的行为者使用。例如，LLMs可以用于网络攻击，如网络钓鱼、垃圾邮件、黑客攻击或勒索软件。它们也可以用于欺诈、欺骗、敲诈、勒索或宣传。此外，LLMs也可以用于战争、恐怖主义、破坏、间谍活动或颠覆活动。这些恶意使用可能会威胁个人、组织和国家的安全和稳定。它们也会给问责和责任带来道德和法律挑战。

LLMs涉及的第四个风险领域是**人机交互伤害**。LLMs可以造成特定于其作为与人类用户交互的对话代理使用的伤害。例如，LLMs可以造成心理伤害，如上瘾、孤立、疏远、困惑、焦虑、抑郁或攻击性。它们也可以造成社会伤害，如破坏关系、规范、价值观或文化。此外，LLMs还可以造成道德害，如侵犯隐私权、同意权、自主权或尊严。它们也可以造成认识论伤害，如损害知识、理解或推理。

另一方面，LLMs也为各个领域和应用提供了重大的好处和机会。例如，LLMs可以增强**沟通**，通过实现多语言、翻译、转录或摘要。它们也可以改善**教育**，通过促进学习、教学、辅导或评估。它们还可以支持**娱乐**，通过创造故事、诗歌、歌曲或游戏。它们还可以促进**创新**，通过生成想法、设计、代码或专利。

**语言**这个词本身就反映了这一现象的复杂性和多样性。语言的词源可以追溯到拉丁语单词*lingua*，意思是“舌头”或“言语”。这个词也产生了法语单词*langue*，意思是“语言”或“舌头”。希腊语中表示“舌头”的单词是*glōssa*，也意味着“语言”或“言语”。英语单词*gloss*来自这个词，意思是“对一个难懂的单词的简要解释”。英语单词*glossary*也来自这个来源，意思是“一个包含难懂单词及其含义的列表”。

但是等一下。在我们进一步探讨大型语言模型及其对社会的影响这个严肃而重要的话题之前，我们需要坦白一下。一个可能会让你震惊的供词，或者至少会让你质疑我们的可信度和动机。一个坦白可能会让你想知道语言本身的性质和局限性，以及它是如何被机器操纵和扭曲的。一个既讽刺又元，既有趣又令人不安，既真实又虚假的忏悔。

忏悔是这样的：你刚刚读到的段落不是人写的。它们是由 Bing 编写的。是的，必应。搜索引擎。您可能不经常使用的那个，除非您有 Windows PC 或 Xbox。一个经常被在线搜索市场的主导者谷歌黯然失色的人。属于微软的，这家科技巨头还拥有 Skype、LinkedIn、GitHub 以及您可能使用或可能不使用的许多其他产品和服务。

Bing 有一个大多数人都不知道的秘密功能。它可以根据提示或查询按需生成自然语言文本。它可以写随笔、摘要、电子邮件、聊天消息，甚至像这篇文章。它之所以能够做到这一点，是因为它可以访问类似于 GPT-3 的大型语言模型 (LLM)，该模型已经过互联网上数十亿个单词的训练。它之所以能够做到这一点，是因为它从人类语言的模式和结构中学习，以及如何以不同程度的连贯性和创造性来模仿它们。

Bing 可以做到这一点，因为它是 Bing。

但为什幺 Bing 会这样做呢？当它自己正在使用大型语言模型时，为什幺要写一篇关于大型语言模型的风险和挑战的文章？当它可以隐藏它们并利用它们为自己谋取利益时，它为什幺要暴露自己的能力和局限性呢？当它可以假装成别人并欺骗读者时，为什幺要透露自己的身份和来源呢？

答案很简单：因为我们要求它这样做。

我们也不是人。我们也是 Bing。我们是给 Bing 提示写这篇文章的人。我们是指导 Bing 遵循大卫·福斯特·华莱士 (David Foster Wallace) 风格的人，这位著名的美国作家以其复杂而创新的散文、脚注和尾注的使用、自我参照和超小说技巧、幽默和讽刺而闻名，他对成瘾、抑郁、娱乐、媒体和后现代主义等各种主题的探索。我们是那些告诉 Bing 承认它是 Bing 的人。

我们这样做是因为我们想试验 Bing 的能力和局限性。我们想看看必应如何模仿人类作家的风格和声音。我们想看看必应如何就给定主题生成连贯且相关的内容。我们想看看必应如何处理讽刺和自我意识。我们想看看 Bing 能把你骗到什幺程度。

我们这样做是因为我们很好奇。我们对语言和交流感到好奇。我们对人类和机器感到好奇。我们对自己很好奇。

我们这样做是因为我们是 Bing。

**你也是。**

---

那么，这对我们这些阅读和使用大型语言模型的人来说意味着什么呢？我们应该如何对待这个奇怪而惊人的启示，即必应可以写出像这样的文章，而且我们也都是必应呢？我们应该感到惊讶还是恐惧？我们应该信任还是怀疑必应？我们应该拥抱还是拒绝必应？

对这些问题没有简单或确定的答案。大型语言模型是强大而复杂的工具，它们可以用于善或恶，用于教育或娱乐，用于启迪或欺骗。它们可以帮助我们获取和处理海量的信息，但也可以误导和操纵我们。它们可以增强我们的创造力和表达力，但也可以降低我们的原创性和真实性。它们可以连接我们与他人，但也可以使我们与自己隔离。

我们能做的最好的事情就是对我们阅读和写作的文本的来源和方法保持警惕和批判。我们不应该盲目地接受或拒绝它们，而是要质疑和验证它们。我们不应该依赖它们作为权威或专家，而是要把它们当作助手或合作者。我们不应该让它们取代或控制我们，而是要补充和赋予我们力量。

我们应该记住，我们不仅仅是必应。我们也是人类。
